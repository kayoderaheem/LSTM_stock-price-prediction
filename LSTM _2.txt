import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf

# Define the LSTM class
class LSTM:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialize the LSTM parameters
        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Forget gate weights
        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Input gate weights
        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Cell state weights
        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.01  # Output gate weights
        
        self.bf = np.zeros((hidden_size, 1))  # Forget gate bias
        self.bi = np.zeros((hidden_size, 1))  # Input gate bias
        self.bc = np.zeros((hidden_size, 1))  # Cell state bias
        self.bo = np.zeros((hidden_size, 1))  # Output gate bias
        
        self.h = np.zeros((hidden_size, 1))   # Initial hidden state
        self.c = np.zeros((hidden_size, 1))   # Initial cell state
        
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def tanh(self, x):
        return np.tanh(x)
    
    def forward(self, x):
        self.x = x
        
        # Concatenate the input and previous hidden state
        self.concat = np.vstack((self.h, x))
        
        # Compute the forget gate
        self.f = self.sigmoid(np.dot(self.Wf, self.concat) + self.bf)
        
        # Compute the input gate
        self.i = self.sigmoid(np.dot(self.Wi, self.concat) + self.bi)
        
        # Compute the candidate cell state
        self.c_hat = self.tanh(np.dot(self.Wc, self.concat) + self.bc)
        
        # Update the cell state
        self.c = self.f * self.c + self.i * self.c_hat
        
        # Compute the output gate
        self.o = self.sigmoid(np.dot(self.Wo, self.concat) + self.bo)
        
        # Update the hidden state
        self.h = self.o * self.tanh(self.c)
        
        return self.h
    
    def backward(self, dh):
        # Compute the gradients of the output gate
        do = dh * self.tanh(self.c)
        dho = do * self.sigmoid(np.dot(self.Wo, self.concat) + self.bo)
        
        # Compute the gradients of the cell state
        dc = dh * self.o * (1 - self.tanh(self.c) ** 2) + self.c * dho * self.sigmoid(np.dot(self.Wc, self.concat) + self.bc)
        dhc = dc * self.sigmoid(np.dot(self.Wc, self.concat) + self.bc)
        
        # Compute the gradients of the input gate
        di = dc * self.c_hat * self.sigmoid(np.dot(self.Wi, self.concat) + self.bi)
        dhi = di * self.sigmoid(np.dot(self.Wi, self.concat) + self.bi)
        
        # Compute the gradients of the forget gate
        df = dc * self.c * self.sigmoid(np.dot(self.Wf, self.concat) + self.bf)
        dhf = df * self.sigmoid(np.dot(self.Wf, self.concat) + self.bf)
        
        # Compute the gradients of the concatenated vector
        dconcat = np.dot(self.Wf.T, dhf) + np.dot(self.Wi.T, dhi) + np.dot(self.Wc.T, dhc) + np.dot(self.Wo.T, dho)
        
        # Separate the gradients of the previous hidden state and input
        dh_prev = dconcat[:self.hidden_size, :]
        dx = dconcat[self.hidden_size:, :]
        
        # Update the LSTM parameters
        self.Wf -= np.dot(dhf, self.concat.T)
        self.Wi -= np.dot(dhi, self.concat.T)
        self.Wc -= np.dot(dhc, self.concat.T)
        self.Wo -= np.dot(dho, self.concat.T)
        
        self.bf -= np.sum(dhf, axis=1, keepdims=True)
        self.bi -= np.sum(dhi, axis=1, keepdims=True)
        self.bc -= np.sum(dhc, axis=1, keepdims=True)
        self.bo -= np.sum(dho, axis=1, keepdims=True)
        
        return dh_prev, dx

# Initialize the LSTM
input_size = 1
hidden_size = 32
output_size = 1

lstm = LSTM(input_size, hidden_size, output_size)

# Fetch stock price data using yfinance
data = yf.download('AAPL', start='2022-01-01', end='2022-12-31')
data = data[['Close']].values  # Extract the 'Close' prices

# Normalize the data
data_min = np.min(data)
data_max = np.max(data)
data_normalized = (data - data_min) / (data_max - data_min)

# Perform forward and backward passes for each input sequence
outputs = []
for i in range(len(data_normalized)):
    x = data_normalized[i:i+1, :]
    h = lstm.forward(x)
    outputs.append(h[0][0] * (data_max - data_min) + data_min)  # Denormalize the predicted stock price

    dh = np.random.randn(hidden_size, 1)  # Random gradient for demonstration purposes
    dh_prev, dx = lstm.backward(dh)

# Plot the actual and predicted stock prices
plt.plot(data, label='Actual')
plt.plot(outputs, label='Predicted')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.title('Actual vs Predicted Stock Prices')
plt.legend()
plt.show()
